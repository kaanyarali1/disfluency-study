{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Automatic Hyperparameter Tuning Results:\n",
        "https://wandb.ai/kaanyarali/speech-test-project-1?workspace=user-kaanyarali"
      ],
      "metadata": {
        "id": "RTwnV2n7z3vg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install !pip install tensorflow_io\n",
        "!pip install keras-tuner\n",
        "!pip install print_schema\n",
        "!pip install pydub\n",
        "!pip install opensmile\n",
        "!pip install tqdm boto3 requests regex sentencepiece sacremoses\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Oyq2X0IXQdE",
        "outputId": "88404b8e-cb4b-40f1-c26c-41a833a7a847"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Invalid requirement: '!pip'\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.7/dist-packages (1.1.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (21.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.23.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.9.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (7.9.0)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.0.4)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (2.0.10)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (5.1.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.2.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.4.2)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.8.0)\n",
            "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.18.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (57.4.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython->keras-tuner) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->keras-tuner) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->keras-tuner) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras-tuner) (3.0.9)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->keras-tuner) (0.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (3.0.4)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.50.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.37.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.4.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.17.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.35.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.9)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->keras-tuner) (4.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras-tuner) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras-tuner) (3.9.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (3.2.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: print_schema in /usr/local/lib/python3.7/dist-packages (1.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from print_schema) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->print_schema) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->print_schema) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->print_schema) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->print_schema) (1.25.11)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: opensmile in /usr/local/lib/python3.7/dist-packages (2.4.1)\n",
            "Requirement already satisfied: audinterface>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from opensmile) (0.9.1)\n",
            "Requirement already satisfied: audobject>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from opensmile) (0.7.5)\n",
            "Requirement already satisfied: audformat<2.0.0,>=0.12.1 in /usr/local/lib/python3.7/dist-packages (from audinterface>=0.7.0->opensmile) (0.14.3)\n",
            "Requirement already satisfied: audresample<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from audinterface>=0.7.0->opensmile) (1.1.0)\n",
            "Requirement already satisfied: oyaml in /usr/local/lib/python3.7/dist-packages (from audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (1.0)\n",
            "Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.7/dist-packages (from audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (6.0)\n",
            "Requirement already satisfied: iso3166 in /usr/local/lib/python3.7/dist-packages (from audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (2.1.1)\n",
            "Requirement already satisfied: audeer<2.0.0,>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (1.18.0)\n",
            "Requirement already satisfied: iso-639 in /usr/local/lib/python3.7/dist-packages (from audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (0.4.5)\n",
            "Requirement already satisfied: pandas!=1.3.0,!=1.3.1,!=1.3.2,!=1.3.3,!=1.4.0,>=1.1.5 in /usr/local/lib/python3.7/dist-packages (from audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (1.3.5)\n",
            "Requirement already satisfied: audiofile>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from audeer<2.0.0,>=1.18.0->audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (4.64.1)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.7/dist-packages (from audiofile>=0.4.0->audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (0.11.0)\n",
            "Requirement already satisfied: sox in /usr/local/lib/python3.7/dist-packages (from audiofile>=0.4.0->audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from audiofile>=0.4.0->audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from audobject>=0.6.1->opensmile) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->audobject>=0.6.1->opensmile) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->audobject>=0.6.1->opensmile) (4.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas!=1.3.0,!=1.3.1,!=1.3.2,!=1.3.3,!=1.4.0,>=1.1.5->audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas!=1.3.0,!=1.3.1,!=1.3.2,!=1.3.3,!=1.4.0,>=1.1.5->audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (2022.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas!=1.3.0,!=1.3.1,!=1.3.2,!=1.3.3,!=1.4.0,>=1.1.5->audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (1.15.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile->audiofile>=0.4.0->audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile->audiofile>=0.4.0->audformat<2.0.0,>=0.12.1->audinterface>=0.7.0->opensmile) (2.21)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2022.6.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.97)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (0.0.53)\n",
            "Requirement already satisfied: botocore<1.29.0,>=1.28.2 in /usr/local/lib/python3.7/dist-packages (from boto3) (1.28.2)\n",
            "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from boto3) (0.6.0)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.29.0,>=1.28.2->boto3) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.29.0,>=1.28.2->boto3) (1.25.11)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.29.0,>=1.28.2->boto3) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.2.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.9.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import gc\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "import pickle"
      ],
      "metadata": {
        "id": "u4-UlwHVSA9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-uncased')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgocDBHva1J2",
        "outputId": "12bb55c1-015f-43a2-ab8d-88a66d9e47bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ADdataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ],
      "metadata": {
        "id": "osDGj7xXXjZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the BertClassfier class\n",
        "class BertClassifier(nn.Module):\n",
        "    \"\"\"Bert Model for Classification Tasks.\n",
        "    \"\"\"\n",
        "    def __init__(self, archs, dropout, freeze_bert=False):\n",
        "        \"\"\"\n",
        "        @param    bert: a BertModel object\n",
        "        @param    classifier: a torch.nn.Module classifier\n",
        "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
        "        \"\"\"\n",
        "        super(BertClassifier, self).__init__()\n",
        "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
        "        D_in = 768\n",
        "\n",
        "        # Instantiate BERT model\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        layers = []          \n",
        "        layers.append(nn.Linear(D_in, archs[0]))\n",
        "        layers.append(nn.BatchNorm1d(num_features=archs[0]))\n",
        "        layers.append(nn.ReLU())\n",
        "        layers.append(nn.Dropout(dropout))\n",
        "\n",
        "        for i in range(1,len(archs)):\n",
        "          layers.append(nn.Linear(archs[i-1], archs[i]))\n",
        "          layers.append(nn.BatchNorm1d(num_features=archs[i]))\n",
        "          layers.append(nn.ReLU())\n",
        "          layers.append(nn.Dropout(dropout))\n",
        "\n",
        "        layers.append(nn.Linear(archs[-1], 1))\n",
        "        self.classifier  = nn.Sequential(*layers)\n",
        "\n",
        "        # Freeze the BERT model\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \"\"\"\n",
        "        Feed input to BERT and the classifier to compute logits.\n",
        "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
        "                      max_length)\n",
        "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
        "                      information with shape (batch_size, max_length)\n",
        "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
        "                      num_labels)\n",
        "        \"\"\"\n",
        "        # Feed input to BERT\n",
        "        outputs = self.bert(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask)\n",
        "        \n",
        "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "        # Feed input to classifier to compute logits\n",
        "        logits = self.classifier(last_hidden_state_cls)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "JZKsN_31Xr_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "def initialize_model(archs, dropout, train_loader, epochs=50):\n",
        "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
        "    \"\"\"\n",
        "    # Instantiate Bert Classifier\n",
        "    bert_classifier = BertClassifier(archs, dropout, freeze_bert=False)\n",
        "\n",
        "    # Tell PyTorch to run the model on GPU\n",
        "    bert_classifier.to(device)\n",
        "\n",
        "    # Create the optimizer\n",
        "    optimizer = AdamW(bert_classifier.parameters(),\n",
        "                      lr=5e-5,    # Default learning rate\n",
        "                      eps=1e-8    # Default epsilon value\n",
        "                      )\n",
        "\n",
        "    # Total number of training steps\n",
        "    total_steps = len(train_loader) * epochs\n",
        "\n",
        "    # Set up the learning rate scheduler\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps=0, # Default value\n",
        "                                                num_training_steps=total_steps)\n",
        "    return bert_classifier, optimizer, scheduler"
      ],
      "metadata": {
        "id": "5KsuWhUPXvpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import time\n",
        "\n",
        "# Specify loss function\n",
        "#loss_fn = nn.CrossEntropyLoss()\n",
        "#loss_fn = nn.BCELoss()\n",
        "loss_fn =  torch.nn.BCEWithLogitsLoss()\n",
        "def set_seed(seed_value=42):\n",
        "    \"\"\"Set seed for reproducibility.\n",
        "    \"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "def train(model, train_loader, optim, val_loader=None, epochs=50, evaluation=False):\n",
        "    \"\"\"Train the BertClassifier model.\n",
        "    \"\"\"\n",
        "    # Start training loop\n",
        "    print(\"Start training...\\n\")\n",
        "    train_loss_list = []\n",
        "    val_loss_list = []\n",
        "    train_acc_list = []\n",
        "    val_acc_list = [] \n",
        "    for epoch_i in range(epochs):\n",
        "        train_loss_sum = 0\n",
        "        train_accuracy_epoch = 0\n",
        "        # Put the model into the training mode\n",
        "        model.train()\n",
        "\n",
        "        # For each batch of training data...\n",
        "        for batch in (train_loader):\n",
        "            optim.zero_grad()\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            # Perform a forward pass. This will return logits.\n",
        "            logits = model(input_ids, attention_mask)\n",
        "\n",
        "            # Compute loss and accumulate the loss values\n",
        "            logits = logits.reshape(-1) #silebilirsin\n",
        "            \n",
        "            loss = loss_fn(logits, labels.float())\n",
        "            train_loss_sum += loss.item()\n",
        "\n",
        "            logits_class = logits > 0.5\n",
        "            train_acc = (labels == logits_class).sum().item() / labels.size(0)\n",
        "            train_accuracy_epoch += train_acc\n",
        "\n",
        "            # Perform a backward pass to calculate gradients\n",
        "            loss.backward()\n",
        "\n",
        "\n",
        "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "\n",
        "            # Update parameters and the learning rate\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "        avg_train_loss = np.round(train_loss_sum/len(train_loader),2)\n",
        "        avg_train_acc = np.round(train_accuracy_epoch/len(train_loader),2)\n",
        "\n",
        "\n",
        "        if evaluation == True:\n",
        "            avg_val_loss, avg_val_acc = evaluate(model, val_loader)\n",
        "        # print('Epoch {}, train loss {} , val loss is {}, train acc is {}, val acc is {} '.format(epoch_i,avg_train_loss,avg_val_loss,avg_train_acc,avg_val_acc))\n",
        "        train_loss_list.append(avg_train_loss)\n",
        "        val_loss_list.append(avg_val_loss)\n",
        "        val_acc_list.append(avg_val_acc)\n",
        "        train_acc_list.append(avg_train_acc)\n",
        "\n",
        "        print(\"Epoch: {} completed. Training Accuracy: {}, Training Loss: {}, Validation Accuracy: {}, Validation Loss: {}\".format(epoch_i,avg_train_acc,avg_train_loss,avg_val_acc,avg_val_loss))\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "    return model,train_loss_list,val_loss_list,train_acc_list,val_acc_list\n",
        "\n",
        "\n",
        "def evaluate(model, val_dataloader):\n",
        "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
        "    on our validation set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_loss_sum = 0\n",
        "    val_accuracy_epoch = 0\n",
        "    # For each batch in our validation set...\n",
        "    for batch in val_dataloader:\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "      labels = batch['labels'].to(device)\n",
        "      \n",
        "      # Compute logits\n",
        "      with torch.no_grad():\n",
        "        logits = model(input_ids, attention_mask)\n",
        "\n",
        "        # Compute loss\n",
        "        logits = logits.reshape(-1)\n",
        "        loss = loss_fn(logits, labels.float())\n",
        "        val_loss_sum +=loss.item()\n",
        "        avg_val_loss = np.round(val_loss_sum/len(val_dataloader),2)\n",
        "\n",
        "        logits_class = logits > 0.5\n",
        "        val_acc = (labels == logits_class).sum().item() / labels.size(0)\n",
        "        val_accuracy_epoch += val_acc\n",
        "        avg_val_acc = np.round(val_accuracy_epoch/len(val_dataloader),2)\n",
        "\n",
        "    return avg_val_loss, avg_val_acc\n",
        "\n",
        "\n",
        "def evaluate_test(model, test_dataloader):\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    test_loss_sum = 0\n",
        "    test_accuracy_epoch = 0\n",
        "    predictions = []\n",
        "    labels_list = []\n",
        "    for batch in test_dataloader:\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "      labels = batch['labels'].to(device)\n",
        "      \n",
        "      # Compute logits\n",
        "      with torch.no_grad():\n",
        "        logits = model(input_ids, attention_mask)\n",
        "        labels_list.append(labels.cpu())\n",
        "\n",
        "        # Compute loss\n",
        "        logits = logits.reshape(-1)\n",
        "        loss = loss_fn(logits, labels.float())\n",
        "        test_loss_sum +=loss.item()\n",
        "        avg_test_loss = np.round(test_loss_sum/len(test_dataloader),5)\n",
        "\n",
        "        logits_class = logits > 0.5\n",
        "        predictions.append(logits.cpu())\n",
        "        test_acc = (labels == logits_class).sum().item() / labels.size(0)\n",
        "        test_accuracy_epoch += test_acc\n",
        "        avg_test_acc = np.round(test_accuracy_epoch/len(test_dataloader),5)\n",
        "\n",
        "    return avg_test_loss, avg_test_acc, predictions, labels_list\n"
      ],
      "metadata": {
        "id": "mFOhbGaKX41z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_ensemble(models, test_dataloader):\n",
        "    test_loss_sum = 0\n",
        "    test_accuracy_epoch = 0\n",
        "    predictions = []\n",
        "    labels_list = []\n",
        "    for batch in test_dataloader:\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "      labels = batch['labels'].to(device)\n",
        "\n",
        "      prediction = []\n",
        "      for model in models:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "          logits = model(input_ids, attention_mask)\n",
        "          labels_list.append(labels.cpu())\n",
        "          logits = logits.reshape(-1)\n",
        "          logits_class = logits > 0.5\n",
        "          prediction.append(logits_class)\n",
        "\n",
        "      prediction_ensemble = sum(prediction) > 0.5*len(prediction)\n",
        "      predictions.append(prediction_ensemble.cpu())\n",
        "\n",
        "      test_acc = (labels == prediction_ensemble).sum().item() / labels.size(0)\n",
        "      test_accuracy_epoch += test_acc\n",
        "      avg_test_acc = np.round(test_accuracy_epoch/len(test_dataloader),5)\n",
        "\n",
        "    return avg_test_acc"
      ],
      "metadata": {
        "id": "BQMbSS-SsJKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "set_seed(seed)    # Set seed for reproducibility"
      ],
      "metadata": {
        "id": "ITK0nQ-bW9iK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltgl9RJBQO8W"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"train.csv\") #load training csv file\n",
        "id = data.iloc[:,0].values\n",
        "train_labels = data.iloc[:,1].values\n",
        "train_texts = data.iloc[:,2].values"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"test.csv\") #load test csv file\n",
        "df[\"Content\"].fillna(\"\",inplace=True)\n",
        "test_texts = list(df.loc[:,\"Content\"])\n",
        "test_labels = list(df.loc[:,\"Label\"])\n",
        "\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
        "test_dataset = ADdataset(test_encodings, test_labels)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "KfHmlJVdSzrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of Samples in the training set: {}\".format(len(train_texts)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNELPW4hSN7m",
        "outputId": "a2979adc-f9e7-4cdc-c72f-a6fbd5eb6b84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Samples in the training set: 166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_several_validation_set(content, label, val_fraction=0.2, total_splits=5, seed=0):\n",
        "  sss = StratifiedShuffleSplit(n_splits=total_splits, test_size=val_fraction, random_state=seed)\n",
        "  return sss.split(content, label)"
      ],
      "metadata": {
        "id": "qBx73fFHUAD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_pos_sample_frac(sample_label):\n",
        "  '''\n",
        "  Returns the ratio of positive samples in the given list\n",
        "  '''\n",
        "  return (sample_label == 1).sum() / sample_label.shape[0]"
      ],
      "metadata": {
        "id": "ov7KjCTTUItT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "index = 1\n",
        "models = [] \n",
        "training_accuracy_list = []\n",
        "validation_accuracy_list = []\n",
        "training_loss_list = []\n",
        "validation_loss_list = []\n",
        "train_and_val_splits = get_several_validation_set(train_texts, train_labels, total_splits=3, seed=seed)\n",
        "for train_index, val_index in train_and_val_splits:\n",
        "\n",
        "    dropout = 0.2 #Tune \n",
        "    archs = [64]  #Tune \n",
        "    batch_size = 16 #Tune \n",
        "    epoch = 10\n",
        "\n",
        "    gc.collect()\n",
        "    train_encodings = tokenizer(list(train_texts[train_index]), truncation=True, padding=True)\n",
        "    val_encodings = tokenizer(list(train_texts[val_index]), truncation=True, padding=True)\n",
        "    train_dataset = ADdataset(train_encodings, train_labels[train_index])\n",
        "    val_dataset = ADdataset(val_encodings, train_labels[val_index])\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    \n",
        "    bert_classifier, optimizer, scheduler = initialize_model(archs, dropout, train_loader, epochs=epoch)\n",
        "    bert_classifier,train_loss_list,val_loss_list,train_acc_list,val_acc_list = train(bert_classifier, train_loader, optimizer, val_loader, epochs=epoch, evaluation=True)\n",
        "\n",
        "    test_loss, test_accuracy, predictions, labels_l = evaluate_test(bert_classifier, test_loader)\n",
        "    print(\"Test Loss: {}, Test Accuracy: {}\".format(test_loss,test_accuracy))\n",
        "    print(\"#######\")\n",
        "   \n",
        "    torch.save(bert_classifier, \"/content/drive/MyDrive/speech_weights/seed_{}_index_{}.pt\".format(seed,index))\n",
        "    with open('/content/drive/MyDrive/speech_traning_logs/training_loss/seed_{}_index_{}'.format(seed,index), 'wb') as fp:\n",
        "      pickle.dump(train_loss_list, fp)\n",
        "    with open('/content/drive/MyDrive/speech_traning_logs/training_accuracy/seed_{}_index_{}'.format(seed,index), 'wb') as fp:\n",
        "      pickle.dump(train_acc_list, fp)\n",
        "    with open('/content/drive/MyDrive/speech_traning_logs/validation_loss/seed_{}_index_{}'.format(seed,index), 'wb') as fp:\n",
        "      pickle.dump(val_loss_list, fp)\n",
        "    with open('/content/drive/MyDrive/speech_traning_logs/validation_accuracy/seed_{}_index_{}'.format(seed,index), 'wb') as fp:\n",
        "      pickle.dump(val_acc_list, fp)\n",
        "\n",
        "    index += 1\n",
        "    models.append(bert_classifier)\n",
        "    training_accuracy_list.append(np.mean(train_acc_list))\n",
        "    validation_accuracy_list.append(np.mean(val_acc_list))\n",
        "    training_loss_list.append(np.mean(train_loss_list))\n",
        "    validation_loss_list.append(np.mean(val_loss_list))\n",
        "\n",
        "    gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SRY5NqFa4Jo",
        "outputId": "7a3951c5-fd3f-4084-f406-24ca2d3b5f1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training...\n",
            "\n",
            "Epoch: 0 completed. Training Accuracy: 0.56, Training Loss: 0.65, Validation Accuracy: 0.5, Validation Loss: 0.63\n",
            "Epoch: 1 completed. Training Accuracy: 0.71, Training Loss: 0.51, Validation Accuracy: 0.71, Validation Loss: 0.6\n",
            "Epoch: 2 completed. Training Accuracy: 0.84, Training Loss: 0.33, Validation Accuracy: 0.81, Validation Loss: 0.41\n",
            "Epoch: 3 completed. Training Accuracy: 0.97, Training Loss: 0.22, Validation Accuracy: 0.69, Validation Loss: 0.99\n",
            "Epoch: 4 completed. Training Accuracy: 1.0, Training Loss: 0.16, Validation Accuracy: 0.9, Validation Loss: 0.37\n",
            "Epoch: 5 completed. Training Accuracy: 0.91, Training Loss: 0.22, Validation Accuracy: 0.65, Validation Loss: 0.8\n",
            "Epoch: 6 completed. Training Accuracy: 1.0, Training Loss: 0.15, Validation Accuracy: 0.88, Validation Loss: 0.3\n",
            "Epoch: 7 completed. Training Accuracy: 1.0, Training Loss: 0.14, Validation Accuracy: 0.85, Validation Loss: 0.41\n",
            "Epoch: 8 completed. Training Accuracy: 0.94, Training Loss: 0.2, Validation Accuracy: 0.85, Validation Loss: 0.41\n",
            "Epoch: 9 completed. Training Accuracy: 1.0, Training Loss: 0.14, Validation Accuracy: 0.85, Validation Loss: 0.39\n",
            "Training complete!\n",
            "Test Loss: 0.56978, Test Accuracy: 0.78036\n",
            "#######\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training...\n",
            "\n",
            "Epoch: 0 completed. Training Accuracy: 0.6, Training Loss: 0.63, Validation Accuracy: 0.4, Validation Loss: 0.72\n",
            "Epoch: 1 completed. Training Accuracy: 0.78, Training Loss: 0.47, Validation Accuracy: 0.81, Validation Loss: 0.53\n",
            "Epoch: 2 completed. Training Accuracy: 0.87, Training Loss: 0.31, Validation Accuracy: 0.58, Validation Loss: 0.72\n",
            "Epoch: 3 completed. Training Accuracy: 0.94, Training Loss: 0.25, Validation Accuracy: 0.67, Validation Loss: 0.8\n",
            "Epoch: 4 completed. Training Accuracy: 0.99, Training Loss: 0.16, Validation Accuracy: 0.75, Validation Loss: 0.51\n",
            "Epoch: 5 completed. Training Accuracy: 1.0, Training Loss: 0.16, Validation Accuracy: 0.79, Validation Loss: 0.5\n",
            "Epoch: 6 completed. Training Accuracy: 0.99, Training Loss: 0.16, Validation Accuracy: 0.81, Validation Loss: 0.61\n",
            "Epoch: 7 completed. Training Accuracy: 1.0, Training Loss: 0.15, Validation Accuracy: 0.79, Validation Loss: 0.54\n",
            "Epoch: 8 completed. Training Accuracy: 1.0, Training Loss: 0.16, Validation Accuracy: 0.79, Validation Loss: 0.5\n",
            "Epoch: 9 completed. Training Accuracy: 0.97, Training Loss: 0.21, Validation Accuracy: 0.81, Validation Loss: 0.52\n",
            "Training complete!\n",
            "Test Loss: 0.48567, Test Accuracy: 0.79286\n",
            "#######\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training...\n",
            "\n",
            "Epoch: 0 completed. Training Accuracy: 0.54, Training Loss: 0.67, Validation Accuracy: 0.33, Validation Loss: 0.63\n",
            "Epoch: 1 completed. Training Accuracy: 0.76, Training Loss: 0.48, Validation Accuracy: 0.65, Validation Loss: 0.62\n",
            "Epoch: 2 completed. Training Accuracy: 0.81, Training Loss: 0.36, Validation Accuracy: 0.88, Validation Loss: 0.33\n",
            "Epoch: 3 completed. Training Accuracy: 0.96, Training Loss: 0.23, Validation Accuracy: 0.73, Validation Loss: 0.69\n",
            "Epoch: 4 completed. Training Accuracy: 0.92, Training Loss: 0.23, Validation Accuracy: 0.85, Validation Loss: 0.42\n",
            "Epoch: 5 completed. Training Accuracy: 0.99, Training Loss: 0.14, Validation Accuracy: 0.73, Validation Loss: 0.78\n",
            "Epoch: 6 completed. Training Accuracy: 1.0, Training Loss: 0.11, Validation Accuracy: 0.92, Validation Loss: 0.41\n",
            "Epoch: 7 completed. Training Accuracy: 1.0, Training Loss: 0.1, Validation Accuracy: 0.88, Validation Loss: 0.33\n",
            "Epoch: 8 completed. Training Accuracy: 1.0, Training Loss: 0.11, Validation Accuracy: 0.88, Validation Loss: 0.35\n",
            "Epoch: 9 completed. Training Accuracy: 0.89, Training Loss: 0.16, Validation Accuracy: 0.88, Validation Loss: 0.36\n",
            "Training complete!\n",
            "Test Loss: 0.39096, Test Accuracy: 0.85893\n",
            "#######\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avg_test_acc = evaluate_ensemble(models, test_loader)\n",
        "print(\"Training Accuracy Average: {}, std: {}\".format(np.mean(training_accuracy_list),np.std(training_accuracy_list)))\n",
        "print(\"Validation Accuracy Average: {}, std: {}\".format(np.mean(validation_accuracy_list),np.std(validation_accuracy_list)))\n",
        "print(\"Training Loss Average: {}, std: {}\".format(np.mean(training_loss_list),np.std(training_loss_list)))\n",
        "print(\"Validation Loss Average: {}, std: {}\".format(np.mean(validation_loss_list),np.std(validation_loss_list)))\n",
        "print(\"Ensemble Test accuracy (Majority Voting): {}\".format(avg_test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6dp2-88sghg",
        "outputId": "a291fd17-5c4a-462c-90df-db04da9b5b96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy Average: 0.898, std: 0.011575836902790201\n",
            "Validation Accuracy Average: 0.7539999999999999, std: 0.024097026095903726\n",
            "Training Loss Average: 0.26566666666666666, std: 0.005312459150169769\n",
            "Validation Loss Average: 0.5393333333333333, std: 0.04246043910381626\n",
            "Ensemble Test accuracy (Majority Voting): 0.87143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter configurations having highest validation accuracy\n",
        "archs = [[512,64],[64,512],[256,512,64],[128],[128,512,256],[256,64],[256,128],[512,256],[256,64,128],[512,256,64],[128],[64,512],[256,512,128]]\n",
        "batch_size = [16,16,8,8,16,16,16,16,16,16,16,16,16]\n",
        "dropout = [0.4,0.5,0.3,0.3,0.4,0.3,0.4,0.5,0.3,0.4,0.5,0.3,0.3]\n",
        "epochs = 15"
      ],
      "metadata": {
        "id": "Z4zx5UiS4WBw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}